{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWNsRgOeG1w2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Assignment: Analyzing Book Genres and Themes through Clustering\n",
        "\n",
        "Full Credit: 20 points\n",
        "\n",
        "You can either work independently or in a group of three. Should you choose to work in a group, you MUST mention the group member names. You can only work in groups of three ie each group should have only 3 members.\n",
        "\n",
        "#### **Objective:**\n",
        "The goal of this assignment is to utilize clustering techniques on a dataset of book descriptions to achieve the following:\n",
        "\n",
        "1. **Discover Common Themes:**\n",
        "   - Identify prevalent themes within clusters of books.\n",
        "2. **Assign Genres to Authors:**\n",
        "   - Group books into clusters and determine the genre for each author based on their cluster membership.\n",
        "\n",
        "#### **Functionality Requirements:**\n",
        "Your program should:\n",
        "- Display the genre associated with a given author. (**10 points**)\n",
        "- Identify and display the common themes of a specific authorâ€™s works. (**5 points**)\n",
        "- Visualize using a Word cloud (**5 points**)\n",
        "---\n",
        "\n",
        "#### **Dataset:**\n",
        "Use the `cleaned_books.csv` dataset that can be downloaded that includes book descriptions and author details.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Tasks:**\n",
        "\n",
        "1. **Data Preprocessing:**\n",
        "   - Load the dataset into a Pandas DataFrame.\n",
        "   - Clean the book descriptions by removing stop words and performing additional necessary preprocessing steps.\n",
        "\n",
        "2. **Clustering:**\n",
        "   - Generate TF-IDF vectors from the book descriptions using `TfidfVectorizer`.\n",
        "   - Identify the optimal number of clusters (`k`) using a suitable method (e.g., the Elbow Method).\n",
        "   - Choose a clustering algorithm (e.g., K-Means, DBSCAN) and justify your choice.\n",
        "   - Perform clustering to group books into clusters and assign a cluster label to each book in the DataFrame.\n",
        "\n",
        "3. **Genre Assignment:**\n",
        "   - Group books by author and cluster labels.\n",
        "   - Assign the most frequent cluster associated with each author as their genre.\n",
        "   - Create a data structure (e.g., a Pandas Series) to store the mapping of authors to their assigned genres.\n",
        "\n",
        "4. **Theme Identification:**\n",
        "   - Develop a function to extract the main themes of a cluster using the top `N` terms from the TF-IDF matrix.\n",
        "   - Apply this function to all clusters to identify and interpret the predominant themes.\n",
        "   - Visualize using a Word cloud\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "```\n",
        "Use CAses: (the below use case is with K Means clustering. You can get different cluster number and different words under themes depending on your implementation. Moreover, you can apply DBScan as well.)\n",
        "\n",
        "Enter an author name: Stephen King\n",
        "The genre of Stephen King is: Cluster 1\n",
        "\n",
        "Common themes for Stephen King: 'concis' 'hemingway' 'autobiograph' ... 'power' 'one' 'world'\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "YMHN8yHbG2Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the following helper code to download the dataset"
      ],
      "metadata": {
        "id": "mhKc0V_oIH50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Download the zip file from the raw URL\n",
        "url = \"https://github.com/sumonacalpoly/Datasets/raw/main/cleaned_books.csv.zip\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Raise an exception for bad responses\n",
        "\n",
        "# Extract the CSV data from the zip file\n",
        "with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
        "    with zf.open('cleaned_books.csv') as f:\n",
        "        cleaned_books = pd.read_csv(f)\n"
      ],
      "metadata": {
        "id": "GGRsJDuRIKw8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = cleaned_books.copy()"
      ],
      "metadata": {
        "id": "99eXEGH4IMCa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "NK89LwwQC-Hu",
        "outputId": "74a18c2d-ec55-4483-d943-0479fcb0c9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.26.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (11.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "dQKsi6QOBF_f",
        "outputId": "944237c0-404c-4f86-e94b-43789e4d289e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "biJsqEpjL22l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}